{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "### In general, the important takeaway from this section for the general practice of data science is matrix multiplication.  \n",
    "\n",
    "#### Being able to answer the questions below will help you quickly debug common errors in code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If matrix A has n rows and m columns, and matrix B has p rows and q columns:\n",
    "What has to be true about the relationship among n, m, p and/or q for A X B to be possible?  For B X A?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In terms of n, m, p and q, what are the numbers of rows and columns in the matrix A X B?  B X A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the concepts of cost and loss\n",
    "    - Cost is a measure of how off your model is from its ability to estimate the relationship of the predicted value and the actual value\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the purpose of gradient descent\n",
    "    - Gradient descent is an algorithm designed to minimize the cost function.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how a cost function and its partial derivitives are used in gradient descent\n",
    "    - gradient descent measures the slope(derivitives) of cost function and follows in a descending gradient until the cost has reached its minimum value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a cost curve\n",
    "    - what it represents, what the axis are, etc\n",
    "        - A cost curve is a plot of Risidual sume squares \n",
    "    \n",
    "    - How each point relates to weights/ coefficients of an algorithm, esp in terms of linear regression\n",
    "    \n",
    "    - how the curve relates to gradient descent, esp in terms of:\n",
    "        - The slope of the curve at any given point\n",
    "        - the interpretationof the point where the curve is 0\n",
    "        - step size\n",
    "        - learning rate\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithems KNN, Logistic Regression, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Be able to talk through:\n",
    "- the abstract process of how the algorithm makes predictions\n",
    "    - Logistic Regression:\n",
    "        - describes and estimates the relationship between one dependant binary variable and independant variables\n",
    "\n",
    "- the advantages the algorithm has over other algorithms on might use in the same situation\n",
    "\n",
    "- what the different hyperparameters are and how they affect the calculationsa of the algorithm\n",
    "\n",
    "- for each hyperparameter, do higher values for that hyperparameter make the model 'more complex' or less complex?  How would you change the hyperparameter if you were trying to fix overfitting?  Underfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Be able to code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fitting a model with that algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating predictions with correcnt methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### log reg predictions\n",
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating metrics( on both the train and validation/ test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### log reg metrics\n",
    "from sklearn import metrics\n",
    "pd.DataFrame(data=[metrics.accuracy_score(y_test, preds), metrics.recall_score(y_test, preds),\n",
    "                   metrics.precision_score(y_test, preds), metrics.f1_score(y_test, preds)], \n",
    "             index=[\"accuracy\", \"recall\", \"precision\", \"F1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Changing / selecting hyperparameter values in response to metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### log reg change threshhold\n",
    "THRESHOLD = 0.75\n",
    "preds = np.where(logreg.predict_proba(X_test)[:,1] > THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the concepts of:\n",
    "- Bias and Variance\n",
    "    - over and under fitting\n",
    "    - Bias and variance\n",
    "    - How vias and variance relate to over and under fitting\n",
    "    - What responses are appropriate to help mitigate over and under-fitting\n",
    "- Validation\n",
    "        - What problem are train test splits designed to help mitigate?\n",
    "        - What problem is cross-validation designed to help mitigate?\n",
    "        - Ho do we interpret train metrics and test metrics to help mitigate those problems?\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code for\n",
    "    - Train-test splits of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8f5b9e76b3b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - metrics of train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion matrix(X_train, y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy Metric\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "# Precision\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred_class))\n",
    "\n",
    "# Recall\n",
    "\n",
    "print(metrics.recall_score(y_test, y_pred_class))\n",
    "\n",
    "#F1\n",
    "2*(metrics.precision_score(y_test,\n",
    "                            y_pred_class)*metrics.recall_score(\n",
    "                            y_test, y_pred_class))/(metrics.precision_score(\n",
    "                            y_test, y_pred_class)+metrics.recall_score(y_test, y_pred_class))\n",
    "\n",
    "### all metrics \n",
    "pd.DataFrame(data=[metrics.accuracy_score(y_test, preds), metrics.recall_score(y_test, preds),\n",
    "                   metrics.precision_score(y_test, preds), metrics.f1_score(y_test, preds)], \n",
    "             index=[\"accuracy\", \"recall\", \"precision\", \"F1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - metrics of cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### be able to interpret a generic confusion matrix:\n",
    "- For any given set of predictions, be able to desgribe:\n",
    "    - What is the 'positive' class and what is the 'negative' class\n",
    "        - Positive class are the instances in a binary classification that have the target attribute, negative class do not have the target attribute.  ex( titanic data.  Alive is positive class and dead is negative class if youa are looking to identify the surviving passangers)\n",
    "    - What the different types of predictions(True positive, fales positive, true negative, and false negative) are.\n",
    "        - True positive: when the model predicts a positive class and the class is positive ex(predicting someone is covid positive when they are covid positive)\n",
    "        - True negative: when the model predicts a negative class and the class is negative ex(predicting someone is covid negative when they are covid negative)\n",
    "        - False Positive: when the model predicts a positive class when the class is actually negative ex(predicting someone is covid positive when they are negative)\n",
    "        - False Negative:  when the model predicts a negative class when the class is actually positive ex(predicting someone is covid negative when they are covid positive)\n",
    "\n",
    "    - Why, in the specific situation in which we are making predictions, we might want to select models that do better minimizing false negatives or false positives\n",
    "        - In the case of covid testing, it is bettr to have false positives vs false negatives bc a false negative patient could spread the the virus to others.  It is better to prevent false negatives in this case to better catagorize true positive cases\n",
    "\n",
    "- Identify which number in the confusion matrix represents tp, tn, fp, and fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Be able to describe in English the below metrics and identify solutions where we might want to select models to do better on that metric:\n",
    "- Accuracy\n",
    "    - How often overall is the prediction model correct\n",
    "- Precision\n",
    "    - When a positive value is predicted, how often is the prediction correct \n",
    "- Recal (Sensitivity)\n",
    "    - what is the proportion of actual correct possitives\n",
    "- F1-score\n",
    "    - The average of precision and recall.  The best f1 being 1 and the worst being 0\n",
    "- specificity\n",
    "    - When the value is negative, how often is the prediction model correct\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Be able to generate and interpret an ROC-AUC graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Be able to describe how class imbalance affects the above metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Be able to describe remideis to class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
